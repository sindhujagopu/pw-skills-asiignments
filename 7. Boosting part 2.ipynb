{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be92f03f",
   "metadata": {},
   "source": [
    "## Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a80614",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression is a machine learning algorithm that belongs to the family of boosting algorithms. It is primarily used for regression tasks, where the goal is to predict a continuous target variable based on a set of input features. Gradient Boosting Regression is an iterative algorithm that combines multiple weak learners (typically decision trees) to create a strong regression model.\n",
    "\n",
    "The main idea behind Gradient Boosting Regression is to iteratively train weak learners, each of which is fitted to the residuals (the differences between the actual target values and the predicted values) of the previous model in the sequence. By repeatedly fitting new models to the residuals, the algorithm gradually reduces the errors and improves the overall prediction accuracy.\n",
    "\n",
    "The working of Gradient Boosting Regression can be summarized as follows:\n",
    "\n",
    "Initialization: The algorithm starts with an initial prediction, which is usually the mean or median of the target variable.\n",
    "\n",
    "Residual Calculation: The residuals are calculated as the differences between the actual target values and the predicted values from the previous model in the sequence.\n",
    "\n",
    "Weak Learner Training: A weak learner, typically a decision tree with limited depth or a stump (a decision tree with a single split), is trained on the residuals. The weak learner aims to capture the patterns or structure in the residuals.\n",
    "\n",
    "Weighted Update: The predictions of the weak learner are multiplied by a learning rate, which controls the contribution of each weak learner to the final prediction. The learning rate helps prevent overfitting and allows for fine-tuning of the ensemble model.\n",
    "\n",
    "Update the Prediction: The new weak learner's prediction is added to the previous prediction, resulting in an updated prediction that is closer to the actual target values.\n",
    "\n",
    "Iteration: Steps 2-5 are repeated iteratively, with each new weak learner trained on the residuals of the previous model. The ensemble model gradually improves its predictive performance by reducing the residuals in each iteration.\n",
    "\n",
    "Final Prediction: The final prediction is obtained by aggregating the predictions of all the weak learners in the ensemble, weighted by their contribution (learning rate). Typically, the final prediction is the sum of the initial prediction and the weighted predictions of each weak learner.\n",
    "\n",
    "Gradient Boosting Regression is known for its ability to handle complex relationships in the data and its effectiveness in capturing non-linear patterns. It has become popular due to its high prediction accuracy and robustness. However, like other boosting algorithms, Gradient Boosting Regression may be susceptible to overfitting if the number of iterations or the complexity of the weak learners is too high. Regularization techniques, such as limiting the depth of the weak learners or using early stopping, can be employed to prevent overfitting and optimize the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da64356",
   "metadata": {},
   "source": [
    "## Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e161abd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 1.4726077357978504\n",
      "R-squared: 0.9989452331221029\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "        self.intercept = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.intercept = np.mean(y)  # Initial prediction is the mean of the target variable\n",
    "\n",
    "        # Iterate over the number of estimators\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Compute the residuals (negative gradient) based on the current predictions\n",
    "            residuals = y - self.predict(X)\n",
    "\n",
    "            # Train a decision tree (weak learner) on the residuals\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "\n",
    "            # Update the prediction by adding the tree's predictions with learning rate\n",
    "            self.models.append(tree)\n",
    "            self.intercept += self.learning_rate * tree.predict(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Compute the predictions by summing the initial intercept and the predictions from each tree\n",
    "        predictions = np.full(X.shape[0], self.intercept)\n",
    "        for tree in self.models:\n",
    "            predictions += self.learning_rate * tree.predict(X)\n",
    "        return predictions\n",
    "\n",
    "# Generate a small regression dataset for demonstration\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.2, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the gradient boosting regressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = gb_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b1fd0d",
   "metadata": {},
   "source": [
    "## Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe01d0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100}\n",
      "Best Mean Squared Error (MSE): 14.162975420394181\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# Create the gradient boosting regressor\n",
    "gb_regressor = GradientBoostingRegressor()\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(gb_regressor, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding performance\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best Mean Squared Error (MSE):\", -grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7045dc75",
   "metadata": {},
   "source": [
    "## Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d9e824",
   "metadata": {},
   "source": [
    "In Gradient Boosting, a weak learner refers to a simple and relatively low-complexity model that performs only slightly better than random guessing. It is the building block or base model used in the ensemble of models created by the Gradient Boosting algorithm.\n",
    "\n",
    "A weak learner is typically a decision tree with shallow depth (e.g., one level or a few levels) or a linear model. The idea behind using weak learners is that even though they may individually have limited predictive power, combining them in an ensemble can result in a strong learner with improved overall predictive performance.\n",
    "\n",
    "The concept of weak learners is integral to the Gradient Boosting algorithm's iterative nature. In each boosting iteration, a new weak learner is trained to capture the patterns or relationships in the residuals (the differences between the actual target values and the predictions of the previous models). By focusing on the errors made by the previous models, the weak learners are designed to correct and improve the overall prediction.\n",
    "\n",
    "The combination of weak learners in Gradient Boosting is achieved by assigning weights to each weak learner's prediction and summing them up. The weights are determined based on how well each weak learner reduces the error or loss function of the ensemble model.\n",
    "\n",
    "The strength of the Gradient Boosting algorithm lies in its ability to sequentially add weak learners, each one learning from the mistakes of the previous models. As more weak learners are added, the ensemble model gradually improves its predictive power and captures more complex relationships in the data.\n",
    "\n",
    "It's worth noting that the term \"weak learner\" is relative and depends on the context. In Gradient Boosting, a weak learner refers to a model that performs slightly better than random guessing, but it can still be a useful predictor when combined with other weak learners in the ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e0a8e6",
   "metadata": {},
   "source": [
    "## Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6da8bb",
   "metadata": {},
   "source": [
    "\n",
    "The intuition behind the Gradient Boosting algorithm can be summarized as follows:\n",
    "\n",
    "Error Reduction through Iterative Learning: The main idea of Gradient Boosting is to iteratively train weak learners, each one focusing on reducing the errors made by the previous models. The algorithm starts with an initial prediction (usually the mean or median of the target variable) and then sequentially adds weak learners to the ensemble.\n",
    "\n",
    "Gradient Descent Optimization: At each boosting iteration, the algorithm computes the negative gradient (the direction of steepest descent) of the loss function with respect to the current ensemble's predictions. The negative gradient represents the residuals or errors between the actual target values and the current predictions. The weak learner is then trained to fit these residuals, aiming to capture the patterns or relationships in the residuals that were not effectively modeled by the previous weak learners.\n",
    "\n",
    "Weighted Contribution of Weak Learners: The predictions of each weak learner are combined with a weighting factor to update the ensemble's predictions. The weights are determined based on how well each weak learner reduces the error or loss function of the ensemble. Typically, a learning rate parameter is introduced to control the contribution of each weak learner, preventing overfitting and allowing for fine-tuning of the ensemble.\n",
    "\n",
    "Additive Nature of the Ensemble: The final prediction of the Gradient Boosting algorithm is the sum of the initial prediction and the weighted predictions of each weak learner. By iteratively adding weak learners that correct the errors made by the previous models, the ensemble gradually improves its predictive power. The ensemble's strength lies in the fact that it combines multiple weak learners to create a strong learner that can capture complex relationships and make accurate predictions.\n",
    "\n",
    "Regularization and Model Complexity: Gradient Boosting allows for regularization techniques to prevent overfitting. By controlling the learning rate, the number of boosting iterations, and the complexity of the weak learners (e.g., maximum depth of decision trees), the algorithm can strike a balance between fitting the training data and avoiding excessive complexity.\n",
    "\n",
    "In summary, the Gradient Boosting algorithm improves prediction accuracy by iteratively adding weak learners that focus on reducing the errors made by the previous models. It leverages gradient descent optimization to guide the training of weak learners and combines their predictions through weighted contributions to create a strong ensemble model. Through this iterative process, the algorithm gradually learns complex relationships in the data and improves its predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8843ff9c",
   "metadata": {},
   "source": [
    "## Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd70819",
   "metadata": {},
   "source": [
    "The Gradient Boosting algorithm builds an ensemble of weak learners in an iterative manner. Here's an overview of how it constructs the ensemble:\n",
    "\n",
    "Initialize the ensemble: The algorithm starts by initializing the ensemble with an initial prediction. This initial prediction can be a simple estimate, such as the mean or median of the target variable.\n",
    "\n",
    "Compute residuals: The algorithm computes the residuals or errors between the actual target values and the predictions of the current ensemble. These residuals represent the part of the target variable that is not captured by the current ensemble model.\n",
    "\n",
    "Train a weak learner: A weak learner, typically a decision tree with limited depth or a linear model, is trained to fit the residuals. The weak learner is trained to minimize the loss function with respect to the residuals. The loss function can be different depending on the task, such as mean squared error (MSE) for regression or log loss for classification.\n",
    "\n",
    "Compute the contribution of the weak learner: The predictions of the trained weak learner are multiplied by a learning rate, which controls the contribution of each weak learner to the ensemble. The learning rate is a hyperparameter that determines the step size of the algorithm and helps prevent overfitting. The contribution of the weak learner is added to the current ensemble's predictions.\n",
    "\n",
    "Update residuals: The algorithm updates the residuals by subtracting the predictions of the weak learner from the current residuals. This focuses the attention of the subsequent weak learners on the remaining errors or residuals.\n",
    "\n",
    "Repeat steps 3-5: Steps 3 to 5 are repeated for a specified number of iterations or until a stopping criterion is met. In each iteration, a new weak learner is trained to fit the updated residuals and its contribution is added to the ensemble.\n",
    "\n",
    "Final prediction: The final prediction of the Gradient Boosting algorithm is the sum of the initial prediction and the weighted contributions of all the weak learners in the ensemble. This ensemble prediction represents the combined output of the weak learners and typically provides a more accurate prediction than any individual weak learner.\n",
    "\n",
    "By sequentially training weak learners to fit the residuals and updating the ensemble's predictions, the Gradient Boosting algorithm iteratively improves the ensemble's performance. Each weak learner focuses on capturing the errors or residuals that were not adequately modeled by the previous weak learners, resulting in a more accurate ensemble prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c9f38a",
   "metadata": {},
   "source": [
    "## Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3ed935",
   "metadata": {},
   "source": [
    "Constructing the mathematical intuition of the Gradient Boosting algorithm involves understanding the key steps and concepts used in the algorithm. Here are the main steps involved:\n",
    "\n",
    "Loss Function: The algorithm begins by defining a loss function that measures the difference between the predicted values and the actual target values. The choice of loss function depends on the specific problem being solved. For example, mean squared error (MSE) is commonly used for regression, and log loss or exponential loss is used for classification.\n",
    "\n",
    "Initialization: The algorithm starts with an initial prediction, typically the mean or median of the target variable. This acts as the \"zeroth\" model in the ensemble.\n",
    "\n",
    "Gradient Descent Optimization: The Gradient Boosting algorithm employs gradient descent optimization to iteratively train weak learners. It computes the negative gradient of the loss function with respect to the current ensemble's predictions. This gradient represents the direction of steepest descent and indicates the changes needed to minimize the loss function.\n",
    "\n",
    "Training Weak Learners: In each boosting iteration, a weak learner is trained to fit the negative gradient (residuals) of the loss function. Weak learners are typically decision trees with shallow depth or linear models. The objective is to find the weak learner that best reduces the residuals and brings the predictions closer to the true values.\n",
    "\n",
    "Weighted Contributions: Each weak learner's predictions are multiplied by a learning rate parameter. This learning rate controls the contribution of each weak learner to the final ensemble. A lower learning rate makes the algorithm more conservative, while a higher learning rate allows for more aggressive updates.\n",
    "\n",
    "Update Ensemble Predictions: The contributions of the weak learners are added to the ensemble's predictions. The ensemble gradually incorporates the predictions from each weak learner, with the learning rate controlling the weight of each contribution.\n",
    "\n",
    "Iterative Process: Steps 3 to 6 are repeated for a specified number of iterations or until a stopping criterion is met. The algorithm continues to train weak learners to fit the residuals, update the ensemble's predictions, and refine the overall prediction.\n",
    "\n",
    "Final Prediction: The final prediction of the Gradient Boosting algorithm is the sum of the initial prediction and the weighted contributions from all the weak learners in the ensemble. This ensemble prediction represents the combined output of the weak learners and typically provides a more accurate prediction than any individual weak learner.\n",
    "\n",
    "By following these steps, the Gradient Boosting algorithm iteratively constructs an ensemble of weak learners, gradually improving the ensemble's performance by minimizing the loss function and capturing complex patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e03265",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8bdf4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa4a36f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a852ea27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
