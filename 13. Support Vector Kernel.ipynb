{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19a4064e",
   "metadata": {},
   "source": [
    "# Q1. What is the relationship between polynomial functions and kernel functions in machine learning algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e601fec8",
   "metadata": {},
   "source": [
    "\n",
    "In machine learning algorithms, polynomial functions and kernel functions are both used to transform the input data into a higher-dimensional feature space. However, they differ in terms of their approaches and mathematical properties.\n",
    "\n",
    "Polynomial functions are a type of feature mapping that transforms the input data into a higher-dimensional space using polynomial terms. For example, a quadratic polynomial function would map a 2-dimensional input space (x, y) into a 3-dimensional space (1, x, y, x^2, xy, y^2). By applying polynomial functions, the algorithm can capture nonlinear relationships between features.\n",
    "\n",
    "On the other hand, kernel functions are a more general concept used in kernel methods, such as Support Vector Machines (SVMs). These methods aim to find a hyperplane that separates the data points of different classes with the maximum margin. Kernel functions provide a way to implicitly perform the transformation into a higher-dimensional space without explicitly calculating the transformed feature vectors. Instead, they define a similarity measure between pairs of data points in the original input space.\n",
    "\n",
    "One common type of kernel function is the polynomial kernel, which is derived from the polynomial feature mapping. The polynomial kernel calculates the similarity between two data points by computing the inner product of their corresponding transformed feature vectors in the higher-dimensional space. By adjusting the degree parameter of the polynomial kernel, one can control the complexity and flexibility of the decision boundary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e3d46d",
   "metadata": {},
   "source": [
    "## Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34d66533",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6c26e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c14c3605",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bcc919b",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(kernel='poly', degree=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a481aad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;poly&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;poly&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='poly')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85b8e8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svm.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34354635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c4c994",
   "metadata": {},
   "source": [
    "## Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d034ed",
   "metadata": {},
   "source": [
    "\n",
    "In Support Vector Regression (SVR), the value of epsilon determines the width of the epsilon-insensitive tube around the regression line. The epsilon-insensitive tube is a region within which errors are considered acceptable and do not contribute to the loss function.\n",
    "\n",
    "When the value of epsilon is increased in SVR, the number of support vectors generally tends to increase. Support vectors are the data points that lie either on or within the epsilon-insensitive tube. As epsilon increases, the tube widens, allowing more data points to fall within it.\n",
    "\n",
    "The number of support vectors is influenced by the trade-off between model complexity and generalization. Increasing epsilon allows for a larger number of support vectors, which can lead to a more flexible model that captures fine-grained patterns in the data. However, it can also increase the risk of overfitting if the number of support vectors becomes too large relative to the size of the dataset.\n",
    "\n",
    "It is important to strike a balance when choosing the value of epsilon in SVR. A smaller value of epsilon constrains the number of support vectors, leading to a simpler model with potentially better generalization. On the other hand, a larger value of epsilon allows for more support vectors, increasing the model's flexibility but potentially increasing the risk of overfitting.\n",
    "\n",
    "The optimal value of epsilon depends on the specific dataset and problem at hand. It is often determined through cross-validation or other hyperparameter tuning techniques to find the value that yields the best performance on unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d941caf8",
   "metadata": {},
   "source": [
    "# Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works and provide examples of when you might want to increase or decrease its value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb51f79d",
   "metadata": {},
   "source": [
    "\n",
    "The choice of kernel function, C parameter, epsilon parameter, and gamma parameter in Support Vector Regression (SVR) significantly affects the performance of the model. Let's discuss each parameter and its impact:\n",
    "\n",
    "Kernel function: SVR uses a kernel function to transform the input space into a higher-dimensional feature space. The choice of kernel function determines the type of transformation applied. Common kernel functions include linear, polynomial, radial basis function (RBF), and sigmoid. The selection of the kernel function depends on the underlying data and the complexity of the relationship between features. For example:\n",
    "\n",
    "Linear kernel: Suitable for linear relationships between features.\n",
    "Polynomial kernel: Suitable when the relationship between features is polynomial.\n",
    "RBF kernel: Appropriate for capturing nonlinear and complex relationships.\n",
    "Sigmoid kernel: Useful for problems where the relationship between features is similar to a sigmoid function.\n",
    "C parameter: The C parameter controls the trade-off between the model's simplicity (smoothness) and its ability to fit the training data. A smaller C value allows for more errors within the epsilon-insensitive tube, leading to a smoother decision boundary and potentially better generalization. Conversely, a larger C value penalizes errors more heavily, resulting in a more complex model that closely fits the training data. You might want to:\n",
    "\n",
    "Increase C: When you suspect that the training data is noisy or when you prioritize fitting the training data closely.\n",
    "Decrease C: When you want a simpler model with better generalization or when the training data has a lot of outliers.\n",
    "Epsilon parameter: The epsilon parameter defines the width of the epsilon-insensitive tube around the regression line. It determines the tolerance for errors. A larger epsilon allows for a wider tube, accepting larger deviations from the target values. Conversely, a smaller epsilon restricts the tube, requiring predictions to be closer to the target values. You might want to:\n",
    "\n",
    "Increase epsilon: When you can tolerate larger errors or when the target values have significant noise.\n",
    "Decrease epsilon: When you require predictions to be closer to the target values or when the data has low noise levels.\n",
    "Gamma parameter: The gamma parameter influences the influence of individual training examples on the SVR model. It defines the reach of the kernel function and determines how far the influence of a single training example extends. A smaller gamma value makes the influence reach farther, resulting in smoother decision boundaries. A larger gamma value makes the influence more localized, potentially creating complex decision boundaries that fit the training data closely. You might want to:\n",
    "\n",
    "Increase gamma: When you suspect that the model should focus more on individual data points or when there are only a few support vectors.\n",
    "Decrease gamma: When you want the model to consider a wider range of data points or when there are many support vectors.\n",
    "The optimal values for these parameters depend on the specific dataset and problem at hand. It is recommended to use techniques like grid search or randomized search to explore different parameter combinations and select the ones that yield the best performance through cross-validation or evaluation on a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a89e21",
   "metadata": {},
   "source": [
    "## Q5. Assignment:\n",
    "## -  Import the necessary libraries and load the dataset.\n",
    "## -  Split the dataset into training and testing set.\n",
    "## - Preprocess the data using any technique of your choice (e.g. scaling, normaliMation\n",
    "## - Create an instance of the SVC classifier and train it on the training data.\n",
    "## -  Use the trained classifier to predict the labels of the testing data.\n",
    "##  - Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy,precision, recall, F1-score.\n",
    "## -  Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomiMedSearchCV to improve its performance\n",
    "## -  Train the tuned classifier on the entire dataset.\n",
    " ## - Save the trained classifier to a file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "119f7b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['svm_classifier.pkl']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "\n",
    "# Load the dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the data by scaling using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create an instance of the SVC classifier\n",
    "svc = SVC()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svc.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Use the trained classifier to predict labels for the testing data\n",
    "y_pred = svc.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the performance of the classifier using accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {'C': [0.1, 1, 10], 'gamma': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}\n",
    "grid_search = GridSearchCV(estimator=svc, param_grid=param_grid, cv=3)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Train the tuned classifier on the entire dataset\n",
    "svc_tuned = grid_search.best_estimator_\n",
    "svc_tuned.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Save the trained classifier to a file\n",
    "joblib.dump(svc_tuned, 'svm_classifier.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec2ac45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2898638a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00047d72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72c5f25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
