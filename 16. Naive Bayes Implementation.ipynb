{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fcc7a46",
   "metadata": {},
   "source": [
    "## Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af8e472",
   "metadata": {},
   "source": [
    "The probability that an employee is a smoker given that he/she uses the health insurance plan is 40%.\n",
    "\n",
    "Code snippet\n",
    "P(Smoker|Uses Plan) = P(Smoker and Uses Plan) / P(Uses Plan)\n",
    "= 0.4 / 0.7\n",
    "= 0.5714285714285714\n",
    "Use code with caution. Learn more\n",
    "This means that if you randomly select an employee who uses the health insurance plan, there is a 57.14% chance that they are a smoker.\n",
    "\n",
    "Here is a breakdown of the probabilities:\n",
    "\n",
    "P(Smoker) = 0.30 (30%)\n",
    "P(Uses Plan) = 0.70 (70%)\n",
    "P(Smoker and Uses Plan) = 0.40 (40%)\n",
    "The probability that an employee is a smoker and uses the health insurance plan is equal to the probability that an employee is a smoker multiplied by the probability that an employee uses the health insurance plan\n",
    "\n",
    "P(Smoker and Uses Plan) = P(Smoker) * P(Uses Plan)\n",
    "= 0.30 * 0.70\n",
    "= 0.21\n",
    "\n",
    "\n",
    "The probability that an employee is a smoker given that he/she uses the health insurance plan is 40%.\n",
    "\n",
    "Code snippet\n",
    "P(Smoker|Uses Plan) = P(Smoker and Uses Plan) / P(Uses Plan)\n",
    "= 0.4 / 0.7\n",
    "= 0.5714285714285714\n",
    "Use code with caution. Learn more\n",
    "This means that if you randomly select an employee who uses the health insurance plan, there is a 57.14% chance that they are a smoker.\n",
    "\n",
    "Here is a breakdown of the probabilities:\n",
    "\n",
    "P(Smoker) = 0.30 (30%)\n",
    "P(Uses Plan) = 0.70 (70%)\n",
    "P(Smoker and Uses Plan) = 0.40 (40%)\n",
    "The probability that an employee is a smoker and uses the health insurance plan is equal to the probability that an employee is a smoker multiplied by the probability that an employee uses the health insurance plan.\n",
    "\n",
    "Code snippet\n",
    "P(Smoker and Uses Plan) = P(Smoker) * P(Uses Plan)\n",
    "= 0.30 * 0.70\n",
    "= 0.21\n",
    "\n",
    "The probability that an employee uses the health insurance plan is given by the survey results.\n",
    "\n",
    "P(Uses Plan) = 0.70\n",
    "\n",
    "The probability that an employee is a smoker given that he/she uses the health insurance plan is calculated by dividing the probability that an employee is a smoker and uses the health insurance plan by the probability that an employee uses the health insurance plan.\n",
    "\n",
    "P(Smoker|Uses Plan) = P(Smoker and Uses Plan) / P(Uses Plan)\n",
    "= 0.21 / 0.70\n",
    "= 0.5714285714285714\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a8cfd9",
   "metadata": {},
   "source": [
    "## Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9814f1",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes and Multinomial Naive Bayes are two variants of the Naive Bayes classifier that are commonly used for different types of data. Here are the key differences between the two:\n",
    "\n",
    "`Feature Type:`\n",
    "\n",
    "- Bernoulli Naive Bayes: It is suitable for binary or Boolean features, where each feature can take only two values, typically representing the presence or absence of a particular attribute.\n",
    "- Multinomial Naive Bayes: It is designed for discrete features that can have multiple values or represent counts or frequencies. It is commonly used in text classification tasks where features are typically word occurrences or frequencies.\n",
    "\n",
    "`Feature Distribution:`\n",
    "\n",
    "- Bernoulli Naive Bayes: It assumes that the features are independent binary variables, and their distribution is modeled using a Bernoulli distribution. Each feature is considered as a binary variable, representing its presence or absence in a particular instance.\n",
    "- Multinomial Naive Bayes: It assumes that the features follow a multinomial distribution. The features are typically treated as counts or frequencies of events in a document or a sample, such as word occurrences or term frequencies.\n",
    "\n",
    "`Handling Absence of Features:`\n",
    "\n",
    "- Bernoulli Naive Bayes: It explicitly models the absence of features and considers it as an informative factor. The absence of a feature contributes to the classification decision.\n",
    "- Multinomial Naive Bayes: It does not explicitly model the absence of features. It focuses on the presence and frequency of features, assuming that the absence of a feature does not provide additional information. \n",
    "\n",
    "`Data Representation:`\n",
    "\n",
    "- Bernoulli Naive Bayes: It often uses a binary feature vector representation, where each feature is represented as a binary value (0 or 1), indicating its presence or absence.\n",
    "- Multinomial Naive Bayes: It typically uses a frequency-based representation, where each feature is represented by its count or frequency in a document or sample.\n",
    "\n",
    "In summary, Bernoulli Naive Bayes is suitable for binary features, explicitly models the absence of features, and follows a Bernoulli distribution. Multinomial Naive Bayes, on the other hand, is used for discrete features or word frequencies, does not explicitly model the absence of features, and assumes a multinomial distribution. The choice between the two depends on the nature of the features and the specific requirements of the classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca411943",
   "metadata": {},
   "source": [
    "## Q3. How does Bernoulli Naive Bayes handle missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549f7b28",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes handles missing values by treating them as a separate category or value for each feature. Instead of assuming the absence or presence of a feature, a missing value is considered as a distinct category or value within the feature.\n",
    "\n",
    "When training a Bernoulli Naive Bayes classifier, the presence or absence of each feature is determined based on the observed data. However, if a missing value is encountered during prediction for a particular feature, the classifier considers it as a separate category and calculates probabilities accordingly.\n",
    "\n",
    "Here's a step-by-step explanation of how Bernoulli Naive Bayes handles missing values:\n",
    "\n",
    "`During training:`\n",
    "\n",
    "- For each feature, the presence or absence of the feature is recorded based on the observed data. If a feature is present in a training instance, its corresponding value is set to 1; otherwise, it is set to 0.\n",
    "\n",
    "`During prediction:`\n",
    "\n",
    "- If a missing value is encountered for a particular feature in the test instance, the classifier treats it as a separate category or value within that feature.\n",
    "\n",
    "- The probability calculations for each class consider the missing value as a distinct category and include it in the calculations.\n",
    "\n",
    "\n",
    "By treating missing values as a separate category, Bernoulli Naive Bayes allows for the inclusion of instances with missing values during the classification process. However, it's important to note that the treatment of missing values in Bernoulli Naive Bayes depends on the specific implementation and may vary in different scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddc52d7",
   "metadata": {},
   "source": [
    "## Q4. Can Gaussian Naive Bayes be used for multi-class classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ac6b97",
   "metadata": {},
   "source": [
    "Yes, Gaussian Naive Bayes can be used for multi-class classification. Gaussian Naive Bayes is a variant of the Naive Bayes classifier that assumes the features within each class are normally distributed (follow a Gaussian distribution). It is commonly used for continuous numerical features.\n",
    "\n",
    "To adapt Gaussian Naive Bayes for multi-class classification, the classifier uses the same underlying principles as in binary classification but extends them to handle multiple classes. The classifier calculates the likelihoods and prior probabilities for each class and then applies Bayes' theorem to calculate the posterior probabilities.\n",
    "\n",
    "The steps for using Gaussian Naive Bayes for multi-class classification are as follows:\n",
    "\n",
    "`Training:`\n",
    "- For each class, calculate the mean and standard deviation of each feature based on the training data belonging to that class.\n",
    "\n",
    "- Estimate the prior probability for each class by calculating the frequency or proportion of training instances belonging to each class.\n",
    "\n",
    "`Prediction:`\n",
    "\n",
    "- Given a new instance with feature values, calculate the likelihoods for each class using the Gaussian probability density function (PDF) based on the mean and standard deviation of each feature in each class.\n",
    "- Multiply the likelihoods by the prior probabilities for each class.\n",
    "- Normalize the probabilities by dividing each class probability by the sum of all class probabilities to obtain posterior probabilities.\n",
    "- The class with the highest posterior probability is predicted as the class label for the new instance.\n",
    "- In multi-class classification with Gaussian Naive Bayes, the model assumes that each feature follows a Gaussian distribution within each class, and the class probabilities are calculated based on these distributions. It makes class predictions based on the posterior probabilities.\n",
    "\n",
    "It's important to note that Gaussian Naive Bayes assumes that the features are conditionally independent given the class label. This is a simplifying assumption and may not hold in all cases. However, despite this assumption, Gaussian Naive Bayes can still perform well in practice, especially when the feature independence assumption is reasonably satisfied or when there are limited training data available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5f7a6a",
   "metadata": {},
   "source": [
    "## Q5. Assignment:\n",
    "### `Data preparation`:\n",
    "- Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message is spam or not based on several input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e48978c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "0            0.00               0.64           0.64           0.0   \n",
       "1            0.21               0.28           0.50           0.0   \n",
       "2            0.06               0.00           0.71           0.0   \n",
       "3            0.00               0.00           0.00           0.0   \n",
       "4            0.00               0.00           0.00           0.0   \n",
       "\n",
       "   word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "0           0.32            0.00              0.00                0.00   \n",
       "1           0.14            0.28              0.21                0.07   \n",
       "2           1.23            0.19              0.19                0.12   \n",
       "3           0.63            0.00              0.31                0.63   \n",
       "4           0.63            0.00              0.31                0.63   \n",
       "\n",
       "   word_freq_order  word_freq_mail  ...  char_freq_;  char_freq_(  \\\n",
       "0             0.00            0.00  ...         0.00        0.000   \n",
       "1             0.00            0.94  ...         0.00        0.132   \n",
       "2             0.64            0.25  ...         0.01        0.143   \n",
       "3             0.31            0.63  ...         0.00        0.137   \n",
       "4             0.31            0.63  ...         0.00        0.135   \n",
       "\n",
       "   char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "0          0.0        0.778        0.000        0.000   \n",
       "1          0.0        0.372        0.180        0.048   \n",
       "2          0.0        0.276        0.184        0.010   \n",
       "3          0.0        0.137        0.000        0.000   \n",
       "4          0.0        0.135        0.000        0.000   \n",
       "\n",
       "   capital_run_length_average  capital_run_length_longest  \\\n",
       "0                       3.756                          61   \n",
       "1                       5.114                         101   \n",
       "2                       9.821                         485   \n",
       "3                       3.537                          40   \n",
       "4                       3.537                          40   \n",
       "\n",
       "   capital_run_length_total  spam  \n",
       "0                       278     1  \n",
       "1                      1028     1  \n",
       "2                      2259     1  \n",
       "3                       191     1  \n",
       "4                       191     1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the feature names\n",
    "feature_names = [\n",
    "    'word_freq_make', 'word_freq_address', 'word_freq_all', 'word_freq_3d', 'word_freq_our',\n",
    "    'word_freq_over', 'word_freq_remove', 'word_freq_internet', 'word_freq_order', 'word_freq_mail',\n",
    "    'word_freq_receive', 'word_freq_will', 'word_freq_people', 'word_freq_report', 'word_freq_addresses',\n",
    "    'word_freq_free', 'word_freq_business', 'word_freq_email', 'word_freq_you', 'word_freq_credit',\n",
    "    'word_freq_your', 'word_freq_font', 'word_freq_000', 'word_freq_money', 'word_freq_hp', 'word_freq_hpl',\n",
    "    'word_freq_george', 'word_freq_650', 'word_freq_lab', 'word_freq_labs', 'word_freq_telnet', 'word_freq_857',\n",
    "    'word_freq_data', 'word_freq_415', 'word_freq_85', 'word_freq_technology', 'word_freq_1999', 'word_freq_parts',\n",
    "    'word_freq_pm', 'word_freq_direct', 'word_freq_cs', 'word_freq_meeting', 'word_freq_original', 'word_freq_project',\n",
    "    'word_freq_re', 'word_freq_edu', 'word_freq_table', 'word_freq_conference', 'char_freq_;', 'char_freq_(', 'char_freq_[',\n",
    "    'char_freq_!', 'char_freq_$', 'char_freq_#', 'capital_run_length_average', 'capital_run_length_longest',\n",
    "    'capital_run_length_total', 'spam'\n",
    "]\n",
    "\n",
    "# Read the spambase.data file into a pandas DataFrame\n",
    "data = pd.read_csv('spambase.data', header=None)\n",
    "\n",
    "# Assign the feature names as column headers\n",
    "data.columns = feature_names\n",
    "\n",
    "# Print the DataFrame\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e54f0b",
   "metadata": {},
   "source": [
    "### `Implementation:`\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "dataset. You should use the default hyperparameters for each classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f9740d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes:\n",
      "Accuracy: 0.8839380364047911\n",
      "\n",
      "Multinomial Naive Bayes:\n",
      "Accuracy: 0.7863496180326323\n",
      "\n",
      "Gaussian Naive Bayes:\n",
      "Accuracy: 0.8217730830896915\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "\n",
    "# Read the spambase.data file into a pandas DataFrame\n",
    "data = pd.read_csv('spambase.data', header=None)\n",
    "\n",
    "# Assign the feature names as column headers\n",
    "data.columns = feature_names\n",
    "\n",
    "# Extract the features and target variable\n",
    "X = data.drop('spam', axis=1)\n",
    "y = data['spam']\n",
    "\n",
    "# Initialize the classifiers\n",
    "bernoulli_nb = BernoulliNB()\n",
    "multinomial_nb = MultinomialNB()\n",
    "gaussian_nb = GaussianNB()\n",
    "\n",
    "# Perform 10-fold cross-validation and evaluate the performance\n",
    "print(\"Bernoulli Naive Bayes:\")\n",
    "bernoulli_scores = cross_val_score(bernoulli_nb, X, y, cv=10)\n",
    "print(\"Accuracy:\", bernoulli_scores.mean())\n",
    "\n",
    "print(\"\\nMultinomial Naive Bayes:\")\n",
    "multinomial_scores = cross_val_score(multinomial_nb, X, y, cv=10)\n",
    "print(\"Accuracy:\", multinomial_scores.mean())\n",
    "\n",
    "print(\"\\nGaussian Naive Bayes:\")\n",
    "gaussian_scores = cross_val_score(gaussian_nb, X, y, cv=10)\n",
    "print(\"Accuracy:\", gaussian_scores.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee9731f",
   "metadata": {},
   "source": [
    "\n",
    "### `Results:`\n",
    "Report the following performance metrics for each classifier:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20e421d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes:\n",
      "Accuracy: 0.8839380364047911\n",
      "Precision: 0.8869617393737383\n",
      "Recall: 0.8152389047416673\n",
      "F1 Score: 0.8481249015095276\n",
      "\n",
      "Multinomial Naive Bayes:\n",
      "Accuracy: 0.7863496180326323\n",
      "Precision: 0.7393175533565436\n",
      "Recall: 0.7214983911116508\n",
      "F1 Score: 0.7282909724016348\n",
      "\n",
      "Gaussian Naive Bayes:\n",
      "Accuracy: 0.8217730830896915\n",
      "Precision: 0.7103733928118492\n",
      "Recall: 0.9569516119239877\n",
      "F1 Score: 0.8130660909542995\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the classifiers\n",
    "bernoulli_nb = BernoulliNB()\n",
    "multinomial_nb = MultinomialNB()\n",
    "gaussian_nb = GaussianNB()\n",
    "\n",
    "# Perform 10-fold cross-validation and evaluate the performance\n",
    "print(\"Bernoulli Naive Bayes:\")\n",
    "bernoulli_accuracy = cross_val_score(bernoulli_nb, X, y, cv=10, scoring='accuracy').mean()\n",
    "bernoulli_precision = cross_val_score(bernoulli_nb, X, y, cv=10, scoring='precision').mean()\n",
    "bernoulli_recall = cross_val_score(bernoulli_nb, X, y, cv=10, scoring='recall').mean()\n",
    "bernoulli_f1 = cross_val_score(bernoulli_nb, X, y, cv=10, scoring='f1').mean()\n",
    "\n",
    "print(\"Accuracy:\", bernoulli_accuracy)\n",
    "print(\"Precision:\", bernoulli_precision)\n",
    "print(\"Recall:\", bernoulli_recall)\n",
    "print(\"F1 Score:\", bernoulli_f1)\n",
    "\n",
    "print(\"\\nMultinomial Naive Bayes:\")\n",
    "multinomial_accuracy = cross_val_score(multinomial_nb, X, y, cv=10, scoring='accuracy').mean()\n",
    "multinomial_precision = cross_val_score(multinomial_nb, X, y, cv=10, scoring='precision').mean()\n",
    "multinomial_recall = cross_val_score(multinomial_nb, X, y, cv=10, scoring='recall').mean()\n",
    "multinomial_f1 = cross_val_score(multinomial_nb, X, y, cv=10, scoring='f1').mean()\n",
    "\n",
    "print(\"Accuracy:\", multinomial_accuracy)\n",
    "print(\"Precision:\", multinomial_precision)\n",
    "print(\"Recall:\", multinomial_recall)\n",
    "print(\"F1 Score:\", multinomial_f1)\n",
    "\n",
    "print(\"\\nGaussian Naive Bayes:\")\n",
    "gaussian_accuracy = cross_val_score(gaussian_nb, X, y, cv=10, scoring='accuracy').mean()\n",
    "gaussian_precision = cross_val_score(gaussian_nb, X, y, cv=10, scoring='precision').mean()\n",
    "gaussian_recall = cross_val_score(gaussian_nb, X, y, cv=10, scoring='recall').mean()\n",
    "gaussian_f1 = cross_val_score(gaussian_nb, X, y, cv=10, scoring='f1').mean()\n",
    "\n",
    "print(\"Accuracy:\", gaussian_accuracy)\n",
    "print(\"Precision:\", gaussian_precision)\n",
    "print(\"Recall:\", gaussian_recall)\n",
    "print(\"F1 Score:\", gaussian_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86aebe5f",
   "metadata": {},
   "source": [
    "### `Discussion:`\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
    "the case? Are there any limitations of Naive Bayes that you observed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924059b0",
   "metadata": {},
   "source": [
    "Based on the accuracy metric alone, Bernoulli Naive Bayes performed the best with an accuracy of 0.8839. It achieved the highest accuracy among the three classifiers. However, it's important to note that accuracy alone may not provide a complete picture of the classifier's performance.\n",
    "\n",
    "The reason Bernoulli Naive Bayes performed well could be due to the nature of the dataset and the assumption of binary features in the Bernoulli model. Since the dataset contains binary features (presence or absence of words), the Bernoulli Naive Bayes model, which assumes binary features, may be a better fit for this specific dataset.\n",
    "\n",
    "On the other hand, Multinomial Naive Bayes, which is commonly used for text classification with discrete features, achieved lower accuracy. This could be because the dataset contains continuous features (e.g., word frequencies) that are not well-suited for the Multinomial Naive Bayes assumption.\n",
    "\n",
    "Gaussian Naive Bayes, which assumes a Gaussian distribution for continuous features, performed reasonably well with an accuracy of 0.8218. However, its precision is lower compared to the other two classifiers. This could be because the dataset may not strictly follow a Gaussian distribution for all features.\n",
    "\n",
    "Limitations of Naive Bayes:\n",
    "\n",
    "Naive Bayes assumes independence between features, which may not hold true in real-world scenarios. This assumption can affect the performance of the classifier.\n",
    "It may struggle with rare feature combinations or unseen data during training.\n",
    "Naive Bayes can be sensitive to irrelevant features or features that are not informative for the target variable.\n",
    "It assumes that all features contribute equally to the classification, which may not be the case in reality.\n",
    "These limitations should be taken into consideration when applying Naive Bayes classifiers and evaluating their performance. It's always important to assess the suitability of the algorithm for the specific dataset and problem at hand.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cec0a8",
   "metadata": {},
   "source": [
    "### `Conclusion:`\n",
    "Summarise your findings and provide some suggestions for future work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5673ab",
   "metadata": {},
   "source": [
    "Suggestions for future work:\n",
    "\n",
    "Feature engineering: Explore different feature engineering techniques to enhance the performance of the classifiers. For example, consider transforming continuous features into binary features or applying different scaling methods to improve the modeling assumptions of the classifiers.\n",
    "\n",
    "Hyperparameter tuning: Perform hyperparameter tuning for each classifier to find optimal parameter settings that can potentially improve their performance.\n",
    "\n",
    "Ensemble methods: Investigate the use of ensemble methods, such as combining multiple Naive Bayes classifiers or using ensemble techniques like bagging or boosting, to further enhance the predictive power of the models.\n",
    "\n",
    "Evaluate other classifiers: Compare the performance of Naive Bayes with other classification algorithms, such as decision trees, support vector machines, or neural networks, to identify the best-performing model for the spambase dataset.\n",
    "\n",
    "Handling class imbalance: If the dataset exhibits class imbalance, apply techniques such as oversampling or undersampling to address the imbalance and improve the performance of the classifiers.\n",
    "\n",
    "Cross-validation strategies: Experiment with different cross-validation strategies, such as stratified cross-validation or leave-one-out cross-validation, to assess the robustness of the models and obtain more reliable performance estimates.\n",
    "\n",
    "Feature selection: Explore feature selection techniques, such as information gain, chi-square test, or recursive feature elimination, to identify the most informative features and reduce the dimensionality of the dataset.\n",
    "\n",
    "By considering these suggestions and further exploring the limitations and potential improvements of Naive Bayes classifiers, future work can lead to enhanced spam email classification performance and potentially contribute to the development of more effective anti-spam systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554d1599",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc96a31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
